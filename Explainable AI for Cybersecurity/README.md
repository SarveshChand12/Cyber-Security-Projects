# Explainable AI for Cybersecurity

This repository contains scripts and resources for implementing Explainable Artificial Intelligence (XAI) techniques in cybersecurity applications. The focus is on enhancing the transparency and interpretability of machine learning models used for malware detection and network traffic analysis.

## Project Overview

The project aims to:

- Develop lightweight Convolutional Neural Network (CNN) models to classify inputs (e.g., malware binaries converted to 2D images or network traffic features) as malicious or benign.
- Apply XAI methods such as SHAP (SHapley Additive exPlanations) and Grad-CAM (Gradient-weighted Class Activation Mapping) to elucidate model decisions.
- Provide an interactive dashboard for visualizing explanations, facilitating better understanding and trust in model predictions.

## Repository Structure

- `model.py`: Defines the `EfficientMalwareDetector` class, a lightweight CNN model for malware detection.
- `grad_cam.py`: Implements the `GradCAM` class for generating visual explanations of CNN decisions.
- `shap_explainer.py`: Contains the `SHAPExplainer` class for computing SHAP values to interpret model predictions.
- `dashboard.py`: Sets up the `XAIDashboard` class, an interactive dashboard using Dash for visualizing SHAP and Grad-CAM explanations.
- `main.py`: The main script that integrates the model, explanation methods, and dashboard for end-to-end execution.

## Setup and Installation

1. **Clone the Repository:**

   ```bash
   git clone https://github.com/SarveshChand12/Cyber-Security-Projects.git
   cd Cyber-Security-Projects/Explainable%20AI%20for%20Cybersecurity
   ```

2. **Create a Virtual Environment:**

   It's recommended to use a virtual environment to manage dependencies.

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows, use venv\Scripts\activate
   ```

3. **Install Dependencies:**

   Ensure you have the following dependencies installed:

   - `torch`: For building and training neural networks.
   - `torchvision`: Provides access to popular datasets, model architectures, and image transformations.
   - `shap`: For SHAP explanations.
   - `dash`: For creating the interactive dashboard.
   - `plotly`: For generating interactive plots in the dashboard.

   You can install these packages using pip:

   ```bash
   pip install torch torchvision shap dash plotly
   ```

## Usage

1. **Prepare Input Data:**

   - **Malware Binaries:** Convert malware binary files into 2D grayscale images. Each byte can be represented as a pixel value, forming an image that reflects the binary structure.
   - **Network Traffic Features:** Extract relevant features from network traffic data, such as packet sizes, protocol types, and timing information.

2. **Run the Main Script:**

   Execute the `main.py` script to start the model training and launch the dashboard:

   ```bash
   python main.py
   ```

   The script will:

   - Train the `EfficientMalwareDetector` model on the provided data.
   - Generate explanations using SHAP and Grad-CAM.
   - Launch an interactive dashboard to visualize the explanations.

3. **Interact with the Dashboard:**

   Access the dashboard through your web browser to explore model predictions and their explanations. The dashboard provides:

   - **SHAP Feature Importance Heatmap:** Displays the most influential features affecting the model's decisions.
   - **Grad-CAM Activation Map:** Highlights critical regions in the input data that the model focuses on when making predictions.

## Key Components

- **EfficientMalwareDetector:** A lightweight CNN model based on SqueezeNet architecture, optimized for binary classification tasks in cybersecurity.
- **GradCAM:** A class that generates visual explanations by highlighting important regions in the input data that influence the model's predictions.
- **SHAPExplainer:** A class that computes SHAP values to provide feature-level explanations of the model's output.
- **XAIDashboard:** An interactive dashboard built with Dash, allowing users to visualize and interact with the explanations generated by SHAP and Grad-CAM.

## Data Requirements

The system expects:

- **Input Data:**
  - **Malware Binaries:** Converted to 2D representations (e.g., grayscale images of binary files).
  - **Network Traffic Features:** Feature vectors representing aspects of network traffic.

- **Sample Format:**
  - **Images:** Tensors with dimensions [Batch, Channels, Height, Width] (e.g., [1, 3, 224, 224]).
  - **Tabular Data:** Feature vectors suitable for SHAP analysis.

For demonstration purposes, the scripts may use synthetic data (random tensors). In real-world applications, replace this with actual datasets such as:

- **Malware Datasets:** EMBER, VirusShare.
- **Network Logs:** CIC-IDS2017, UNSW-NB15.

## Outcomes

This project aims to:

- Enhance the interpretability of AI models in cybersecurity.
- Provide tools for cybersecurity analysts to understand and trust model predictions.
- Facilitate integration with Security Information and Event Management (SIEM) systems for real-time threat analysis.

By leveraging XAI techniques, the project seeks to bridge the gap between complex AI models and the need for transparency in cybersecurity applications.

---

*Note: This README provides an overview of the Explainable AI for Cybersecurity project. For detailed information, please refer to the individual scripts and their documentation within the repository.* 